{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import datetime\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from transformers import BertTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "#from reddit_preprocessing import MAX_SEQ_LENGTH, AHOLE_CLASSES, load_dataset"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2022-05-11T13:42:40.380430Z",
     "iopub.execute_input": "2022-05-11T13:42:40.380743Z",
     "iopub.status.idle": "2022-05-11T13:42:46.787769Z",
     "shell.execute_reply.started": "2022-05-11T13:42:40.380663Z",
     "shell.execute_reply": "2022-05-11T13:42:46.787051Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-11T13:42:46.789475Z",
     "iopub.execute_input": "2022-05-11T13:42:46.789722Z",
     "iopub.status.idle": "2022-05-11T13:42:46.857863Z",
     "shell.execute_reply.started": "2022-05-11T13:42:46.789691Z",
     "shell.execute_reply": "2022-05-11T13:42:46.857011Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-11T13:42:46.859110Z",
     "iopub.execute_input": "2022-05-11T13:42:46.859598Z",
     "iopub.status.idle": "2022-05-11T13:42:47.912943Z",
     "shell.execute_reply.started": "2022-05-11T13:42:46.859568Z",
     "shell.execute_reply": "2022-05-11T13:42:47.912222Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "URL_REGEX = '(http|https)://[-a-zA-Z0-9+&@#/%?=~_|!:,.;]*[-a-zA-Z0-9+&@#/%=~_|]'\n",
    "NON_ALPHA_NUMERIC_REGEX = '[^a-zA-Z0-9- ]'\n",
    "TAG_REGEX = '@[^\\\\s]*'\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "AHOLE_CLASSES = ['yta', 'nta']\n",
    "MAX_SEQ_LENGTH = 64\n",
    "\n",
    "\n",
    "def load_dataset(filepath: str, classes: list, tokenizer, rm_punct: bool = False):\n",
    "    texts, labels = [], []\n",
    "    df = pd.read_csv(filepath)\n",
    "    df = df.head(50)\n",
    "    \n",
    "    df_yta = df[df['is_yta'] == 'yta']\n",
    "    df_nta = df[df['is_yta'] == 'nta']\n",
    "    #n = df_yta.shape[0]\n",
    "    #df_nta = df_nta.iloc[:n]\n",
    "    \n",
    "    #df = pd.concat([df_yta, df_nta], axis=0)\n",
    "    print(df)\n",
    "\n",
    "    \n",
    "    df['body'] = df['body'].astype(\"str\")\n",
    "    df['is_yta'] = df['is_yta'] == \"yta\"\n",
    "    for index, row in df.iterrows():\n",
    "        label = row['is_yta']\n",
    "        text = row['body']\n",
    "        tokens = process_text(text, tokenizer, rm_punct)\n",
    "        if len(tokens) > 3:\n",
    "            texts.append(tokens)\n",
    "            labels.append([label, not label])\n",
    "\n",
    "    return texts, labels\n",
    "def process_text(text, tokenizer, rm_punct: bool = False):\n",
    "    if type(text) is not str:\n",
    "        raise TypeError('Text is not of type string')\n",
    "\n",
    "    # remove special string from the text: URLs and emojis (by encoding and decoding to/from ascii)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_tags(text)\n",
    "\n",
    "    # remove unknown characters\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    # remove all punctuation\n",
    "    if rm_punct:\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # remove all labels from texts\n",
    "    tokens = delete_label_word(tokens, 'YTA')\n",
    "    tokens = delete_label_word(tokens, 'NTA')\n",
    "\n",
    "    tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def remove_non_alphanumeric(text: str) -> str:\n",
    "    return re.sub(NON_ALPHA_NUMERIC_REGEX, ' ', text)\n",
    "\n",
    "\n",
    "def remove_urls(text: str) -> str:\n",
    "    return re.sub(URL_REGEX, ' ', text)\n",
    "\n",
    "\n",
    "def remove_tags(text: str) -> str:\n",
    "    return re.sub(TAG_REGEX, ' ', text)\n",
    "\n",
    "\n",
    "def delete_label_word(words: list, label: str) -> list:\n",
    "    while label in words:\n",
    "        words.remove(label)\n",
    "    return words"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-11T13:42:47.915438Z",
     "iopub.execute_input": "2022-05-11T13:42:47.915814Z",
     "iopub.status.idle": "2022-05-11T13:42:47.931167Z",
     "shell.execute_reply.started": "2022-05-11T13:42:47.915773Z",
     "shell.execute_reply": "2022-05-11T13:42:47.930481Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "texts, labels = load_dataset(filepath='/Users/teodorareu/PycharmProjects/COMP34812_k01092tr/data/posts.csv',\n",
    "                             classes=AHOLE_CLASSES,\n",
    "                             tokenizer=tokenizer,\n",
    "                             rm_punct=True)\n",
    "yta_nr = labels.count([True,False])\n",
    "nta_nr = len(labels) - yta_nr\n",
    "print(yta_nr)\n",
    "print(nta_nr)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-11T13:42:47.932623Z",
     "iopub.execute_input": "2022-05-11T13:42:47.932911Z",
     "iopub.status.idle": "2022-05-11T13:43:14.735776Z",
     "shell.execute_reply.started": "2022-05-11T13:42:47.932875Z",
     "shell.execute_reply": "2022-05-11T13:43:14.735027Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                              title  \\\n",
      "0   eiawts  AITA for Not Letting My Friend Invite Her Auti...   \n",
      "1   eiaw8a  WIBTA if I don't pick my cousin up tonight fro...   \n",
      "2   eiav15                      AITA for disowning my family?   \n",
      "3   eiau2i  AITA for “pulling rank” on someone who was giv...   \n",
      "4   eiatrs  AITA for sleeping on the same bed with my friend?   \n",
      "5   eiasoj  AITA for freaking out on my mom for not signin...   \n",
      "6   eias5l     AITA for ghosting a guy that has a crush on me   \n",
      "7   eiaoo9  AITA for telling my family that my sister woul...   \n",
      "8   eiagfd   AITA for cutting ties with an unreliable friend?   \n",
      "9   eiafwb  AITA for not paying my cousin back right away ...   \n",
      "10  eiae06  AITA for not having my mother not in my child'...   \n",
      "11  eiacb2  AITA for cutting off my aunt for spreading mal...   \n",
      "12  eiaady  AITA for having a secret credit card that I hi...   \n",
      "13  eia7fa  AITA for not wanting to have restrictions from...   \n",
      "14  eia774  AITA for making someone feel racially stereoty...   \n",
      "15  eia6ua  AITA for getting my mother-in-law a job with m...   \n",
      "16  eia56a  AITA for not wanting to go to a NYE marriage p...   \n",
      "17  eia1i1  AITA for not wanting to party with my friends ...   \n",
      "18  ei9zka  WIBTA for cutting off a friend for not invitin...   \n",
      "19  ei9rpo                        AITA for lying to my friend   \n",
      "20  ei9qjt  AITA for getting a homeless guy kicked out of ...   \n",
      "21  ei9od9  AITA for confronting my housemate/friend about...   \n",
      "22  ei9jb0  AITA for not reminding an employee to change h...   \n",
      "23  ei9hb1  AITA for flirting with my mom when she was on ...   \n",
      "24  ei9gfs         AITA for calling out a fitness influencer?   \n",
      "25  ei96rm  AITA for *accidentally* hitting my younger sis...   \n",
      "26  ei8zfv  AITA for not letting my fiancé stay out tonigh...   \n",
      "27  ei8z50            AITA for not babysitting roommates kid.   \n",
      "28  ei8w4q             AITA - Entitled wife of selfish hubby?   \n",
      "29  ei8w32                 WIBTA if I took my sister's phone?   \n",
      "30  ei8vft         AITA for not telling my dad happy birthday   \n",
      "31  ei8rhj  AITA for being upset with my husband for going...   \n",
      "32  ei8qjq  AITA for gifting expensive items while we rece...   \n",
      "33  ei8qdm  AITA for cutting my sister out of my life afte...   \n",
      "34  ei8pxh  AITA for being exasperated after getting yelle...   \n",
      "35  ei8pws         AITA in this exchange between my ex and I?   \n",
      "36  ei8ph3  WIBTA for taking money meant to be shared amon...   \n",
      "37  ei8mdy  AITA for telling my cousin if he doesn't want ...   \n",
      "38  ei8m59  WIBTA for not telling my mom her Canada Goose ...   \n",
      "39  ei8lvz  AITA for telling my in-laws to fuck off when t...   \n",
      "40  ei8lhb  AITA for not playing nice with a guy who makes...   \n",
      "41  ei8g7p  AITA For Not Selling a Garage Sale Item to a n...   \n",
      "42  ei894j  AITA for cancelling my New Years Eve dinner wi...   \n",
      "43  ei8921  AITA for getting mad at my GF for grinding on ...   \n",
      "44  ei86qy  AITA for turning down a shift thus resulting i...   \n",
      "45  ei83ij  AITA because I'll cook separate meals sometime...   \n",
      "46  ei7zdu                       AITA for donating his stuff?   \n",
      "47  ei7t60  AITA Having to clean house while visiting for ...   \n",
      "48  ei7sib  AITA for wanting to enjoy my high school freedom?   \n",
      "49  ei7r34  WIBTA if asked someone I'm seeing to wear make...   \n",
      "\n",
      "                                                 body is_yta  \n",
      "0   I'm probably the asshole here, but I figure ma...    nta  \n",
      "1   EDIT: all right, all right. I let you down Red...    yta  \n",
      "2   Sorry this is long\\n\\nMy parents divorced when...    nta  \n",
      "3   I’ll try to keep this short. I’m a lawyer. My ...    nta  \n",
      "4   So to preface I’m a girl(22F) my friend that I...    yta  \n",
      "5   Context: I’m a 15m who lives in a decently siz...    nta  \n",
      "6   i’m on mobile\\n\\nso, i (16f) have a friend (16...    nta  \n",
      "7   My sister is doing dual enrollment classes fro...    nta  \n",
      "8   So I’m going to give some context on a few thi...    nta  \n",
      "9   I got a little crazy at Xmas and bought way to...    nta  \n",
      "10   I got married on Halloween ((it's on Reddit i...    nta  \n",
      "11  Sorry for any formatting, I’m on mobile.\\n\\nSo...    nta  \n",
      "12  I'm 19 and currently a community college stude...    nta  \n",
      "13  So here's a little backstory, I'm a homeschool...    nta  \n",
      "14  I (29F) work at a technology company that is o...    nta  \n",
      "15  My mother-in-law & I, up until recently have b...    nta  \n",
      "16  My (24M) gf (24f) wants me to go to a proposal...    nta  \n",
      "17  Throwaway because one of the friends in the gr...    nta  \n",
      "18  Obligatory mobile warning \\n\\nI’ve had the sam...    nta  \n",
      "19  My friend asked me to tell a girl he liked tha...    yta  \n",
      "20  My mother's friend was homeless, so she let hi...    nta  \n",
      "21  **Context** I (M/30) live with one of my best ...    nta  \n",
      "22  So I do payroll and employees will turn in a W...    yta  \n",
      "23  22M. My mom recently had to have some minor su...    nta  \n",
      "24  Apologies for mobile formatting issues\\n\\n\\nSo...    nta  \n",
      "25  For some context, we live in a small apartment...    nta  \n",
      "26  I just had a phone called from my fiancé sayin...    nta  \n",
      "27  Info: I work from home and he's a mover and co...    nta  \n",
      "28  I’ll try to make this brief while providing as...    yta  \n",
      "29  Sorry for spelling, I'm on mobile + English is...    nta  \n",
      "30  To start, my relationship with my parents in t...    nta  \n",
      "31  My husband is a chef and we have 3 small child...    nta  \n",
      "32  We invited a few friends and families for the ...    yta  \n",
      "33  Me and my sister have never been close, we kep...    nta  \n",
      "34  My GF (24F) of 3 years and I (28M) live in a s...    nta  \n",
      "35  I’m a girl with Asperger’s who came from a gas...    nta  \n",
      "36  My dad is abusive and no longer lives in the h...    yta  \n",
      "37  My family is very non confrontational. Most of...    nta  \n",
      "38  My mom and I bought secondhand Canada Goose ja...    nta  \n",
      "39  My in laws are upper middle class suburbanites...    nta  \n",
      "40  Hello reddit, usually I just kind of lurk arou...    nta  \n",
      "41  Using a throwaway, will try to keep this brief...    nta  \n",
      "42  So my boyfriend and I have been together for f...    nta  \n",
      "43  About two weeks ago my GF of 5 years went out ...    nta  \n",
      "44  3 days ago my boss came and asked me if I coul...    nta  \n",
      "45  I believe my son is being unreasonable about t...    nta  \n",
      "46  So I’ve broken up with my now ex boyfriend abo...    nta  \n",
      "47  22 f and no longer living at home. I've lived ...    nta  \n",
      "48  Title might sound a bit biased, but I’ll expla...    nta  \n",
      "49  I can hear the pitch forks being sharpened but...    yta  \n",
      "8\n",
      "42\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "input_ids, attention_masks = [], []\n",
    "for text in texts:\n",
    "    encoded_dict = tokenizer.encode_plus(text,\n",
    "                                         add_special_tokens=False,\n",
    "                                         truncation=True,\n",
    "                                         max_length=MAX_SEQ_LENGTH,\n",
    "                                         pad_to_max_length=True,\n",
    "                                         return_attention_mask=True,\n",
    "                                         return_tensors='pt')\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-11T13:43:14.736939Z",
     "iopub.execute_input": "2022-05-11T13:43:14.737606Z",
     "iopub.status.idle": "2022-05-11T13:43:16.626693Z",
     "shell.execute_reply.started": "2022-05-11T13:43:14.737566Z",
     "shell.execute_reply": "2022-05-11T13:43:16.625957Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/teodorareu/opt/anaconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-11T13:43:16.628096Z",
     "iopub.execute_input": "2022-05-11T13:43:16.628341Z",
     "iopub.status.idle": "2022-05-11T13:43:16.669415Z",
     "shell.execute_reply.started": "2022-05-11T13:43:16.628307Z",
     "shell.execute_reply": "2022-05-11T13:43:16.668708Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "labels = labels.bool().int().float()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-05-11T13:43:16.670535Z",
     "iopub.execute_input": "2022-05-11T13:43:16.670778Z",
     "iopub.status.idle": "2022-05-11T13:43:16.677343Z",
     "shell.execute_reply.started": "2022-05-11T13:43:16.670746Z",
     "shell.execute_reply": "2022-05-11T13:43:16.676622Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "input_ids.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-05-11T13:43:16.678477Z",
     "iopub.execute_input": "2022-05-11T13:43:16.678727Z",
     "iopub.status.idle": "2022-05-11T13:43:16.688411Z",
     "shell.execute_reply.started": "2022-05-11T13:43:16.678693Z",
     "shell.execute_reply": "2022-05-11T13:43:16.687547Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([50, 64])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "train_size = int(0.75 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,  # The training samples.\n",
    "                              sampler=RandomSampler(train_dataset),  # Select batches randomly\n",
    "                              batch_size=batch_size)\n",
    "\n",
    "validation_dataloader = DataLoader(val_dataset,  # The validation samples.\n",
    "                                   sampler=SequentialSampler(val_dataset),  # Pull out batches sequentially.\n",
    "                                   batch_size=batch_size)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-11T13:43:16.691587Z",
     "iopub.execute_input": "2022-05-11T13:43:16.691943Z",
     "iopub.status.idle": "2022-05-11T13:43:16.706179Z",
     "shell.execute_reply.started": "2022-05-11T13:43:16.691907Z",
     "shell.execute_reply": "2022-05-11T13:43:16.705042Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   37 training samples\n",
      "   13 validation samples\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "validation_dataloader.dataset[3]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-05-11T13:43:16.707524Z",
     "iopub.execute_input": "2022-05-11T13:43:16.707763Z",
     "iopub.status.idle": "2022-05-11T13:43:16.754551Z",
     "shell.execute_reply.started": "2022-05-11T13:43:16.707732Z",
     "shell.execute_reply": "2022-05-11T13:43:16.753743Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([  101,  2061,  2182,  2015,  1037,  2210, 10457,  7062, 10047,  1037,\n          5014,  9905,  9890,  2094,  4845,  2026,  2972,  2166,  2287,  2385,\n          1045,  2147,  2012, 14556, 10882,  2140, 18887,  2031,  5552,  2039,\n          1037,  3232,  4595,  6363,  2013,  2147,  1045,  2525,  2031,  4149,\n          2870,  1037,  6942,  1998,  1037, 26381,  2029,  2026,  3008,  9038,\n          4352,  2033,  2000,  4965,  2055,  1018,  2706,  3283,  1045,  2787,\n          1045,  2359,  1037, 10355]),\n tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n tensor([0., 1.]))"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "from models import WeightedBCEBert\n",
    "\n",
    "model = WeightedBCEBert()\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=2e-5,  # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps=1e-8  # args.adam_epsilon  - default is 1e-8.\n",
    "                  )"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-11T13:43:16.756524Z",
     "iopub.execute_input": "2022-05-11T13:43:16.757157Z",
     "iopub.status.idle": "2022-05-11T13:43:33.235420Z",
     "shell.execute_reply.started": "2022-05-11T13:43:16.757118Z",
     "shell.execute_reply": "2022-05-11T13:43:33.234568Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/teodorareu/opt/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "epochs = 4\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=100,\n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten().astype(np.float32)\n",
    "    labels_flat = labels[:,1].flatten().astype(np.float32)\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-11T13:45:15.261297Z",
     "iopub.execute_input": "2022-05-11T13:45:15.261551Z",
     "iopub.status.idle": "2022-05-11T13:45:15.273525Z",
     "shell.execute_reply.started": "2022-05-11T13:45:15.261523Z",
     "shell.execute_reply": "2022-05-11T13:45:15.272828Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "p = torch.arange(64, dtype=torch.int64).unsqueeze(0)\n",
    "l = torch.ones(64).unsqueeze(0)\n",
    "weights = torch.Tensor([1,1])\n",
    "labels = torch.ones(1,2)\n",
    "m = model(p, l, None, labels, weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.8414, grad_fn=<BinaryCrossEntropyWithLogitsBackward>), logits=tensor([[-0.0447, -0.4886]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(m)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TRAINING\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "#model.cuda()\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "training_stats = []\n",
    "total_t0 = time.time()\n",
    "\n",
    "best_val_acc, best_val_loss = 0, torch.finfo(torch.float32).max\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "\n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "    total_train_accuracy = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 400 batches.\n",
    "        if step % 10 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader.\n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the\n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids\n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "        b_weights = torch.Tensor([yta_nr, nta_nr])\n",
    "        outputs = model(b_input_ids,b_input_mask, None, b_labels,b_weights)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculate the accuracy for this batch of train sentences, and accumulate it over all batches.\n",
    "        total_train_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Report the final accuracy for this train run.\n",
    "    avg_train_accuracy = total_train_accuracy / len(train_dataloader)\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Training accuracy: {0:.4f}\".format(avg_train_accuracy))\n",
    "    print(\"  Average training loss: {0:.4f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode -- the dropout layers behave differently during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        # Unpack this training batch from our dataloader.\n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using\n",
    "        # the `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids\n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which\n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here:\n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            model.zero_grad()\n",
    "            b_weights = torch.Tensor([yta_nr, nta_nr])\n",
    "            outputs = model(b_input_ids,b_input_mask, None, b_labels,b_weights)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "\n",
    "    # save the model with the best accuracy and minimal loss (so we do not save an overfitted model)\n",
    "    if avg_val_accuracy > best_val_acc and avg_val_loss < best_val_loss:\n",
    "        best_val_acc = avg_val_accuracy\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_epoch = epoch_i\n",
    "\n",
    "    print(\"  Validation Acc.: {0:.4f}\".format(avg_val_accuracy))\n",
    "    print(\"  Validation Loss: {0:.4f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Training Acc.': avg_train_accuracy * 100,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Acc.': avg_val_accuracy * 100,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time() - total_t0)))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-11T13:45:17.715304Z",
     "iopub.execute_input": "2022-05-11T13:45:17.715563Z",
     "iopub.status.idle": "2022-05-11T13:56:22.642272Z",
     "shell.execute_reply.started": "2022-05-11T13:45:17.715534Z",
     "shell.execute_reply": "2022-05-11T13:56:22.641507Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    10  of     19.    Elapsed: 0:00:16.\n",
      "\n",
      "  Training accuracy: 0.3684\n",
      "  Average training loss: 18.1945\n",
      "  Training epcoh took: 0:00:29\n",
      "\n",
      "Running Validation...\n",
      "  Validation Acc.: 0.6429\n",
      "  Validation Loss: 10.1823\n",
      "  Validation took: 0:00:02\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    10  of     19.    Elapsed: 0:00:16.\n",
      "\n",
      "  Training accuracy: 0.8947\n",
      "  Average training loss: 11.4014\n",
      "  Training epcoh took: 0:00:30\n",
      "\n",
      "Running Validation...\n",
      "  Validation Acc.: 0.6429\n",
      "  Validation Loss: 7.1095\n",
      "  Validation took: 0:00:02\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    10  of     19.    Elapsed: 0:00:16.\n",
      "\n",
      "  Training accuracy: 0.9211\n",
      "  Average training loss: 4.9715\n",
      "  Training epcoh took: 0:00:30\n",
      "\n",
      "Running Validation...\n",
      "  Validation Acc.: 0.6429\n",
      "  Validation Loss: 2.6536\n",
      "  Validation took: 0:00:02\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    10  of     19.    Elapsed: 0:00:16.\n",
      "\n",
      "  Training accuracy: 0.9211\n",
      "  Average training loss: 1.7566\n",
      "  Training epcoh took: 0:00:28\n",
      "\n",
      "Running Validation...\n",
      "  Validation Acc.: 0.6429\n",
      "  Validation Loss: 2.1331\n",
      "  Validation took: 0:00:02\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:02:06 (h:mm:ss)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "best_model.save_pretrained('/models')\n",
    "print('Best fit model saved at epoch %d' % (best_epoch + 1))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-11T15:12:18.801512Z",
     "iopub.execute_input": "2022-05-11T15:12:18.802256Z",
     "iopub.status.idle": "2022-05-11T15:12:18.907445Z",
     "shell.execute_reply.started": "2022-05-11T15:12:18.802133Z",
     "shell.execute_reply": "2022-05-11T15:12:18.905957Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./best_model_posts.pt\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Display floats with two decimal places.\n",
    "pd.set_option('precision', 4)\n",
    "\n",
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "# A hack to force the column headers to wrap.\n",
    "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "# Display the table.\n",
    "df_stats"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-11T15:12:18.909044Z",
     "iopub.status.idle": "2022-05-11T15:12:18.909834Z",
     "shell.execute_reply.started": "2022-05-11T15:12:18.909481Z",
     "shell.execute_reply": "2022-05-11T15:12:18.909515Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training Loss\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation Loss\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks(list(range(1, epochs + 1)))\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-11T13:45:12.178548Z",
     "iopub.status.idle": "2022-05-11T13:45:12.179240Z",
     "shell.execute_reply.started": "2022-05-11T13:45:12.178973Z",
     "shell.execute_reply": "2022-05-11T13:45:12.179005Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}