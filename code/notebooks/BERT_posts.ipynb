{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import datetime\n",
    "import random\n",
    "import time\n",
    "from aita.models import LSTMW2V\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from transformers import BertTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "#from reddit_preprocessing import MAX_SEQ_LENGTH, AHOLE_CLASSES, load_dataset"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2022-05-11T13:42:40.380430Z",
     "iopub.execute_input": "2022-05-11T13:42:40.380743Z",
     "iopub.status.idle": "2022-05-11T13:42:46.787769Z",
     "shell.execute_reply.started": "2022-05-11T13:42:40.380663Z",
     "shell.execute_reply": "2022-05-11T13:42:46.787051Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-11T13:42:46.789475Z",
     "iopub.execute_input": "2022-05-11T13:42:46.789722Z",
     "iopub.status.idle": "2022-05-11T13:42:46.857863Z",
     "shell.execute_reply.started": "2022-05-11T13:42:46.789691Z",
     "shell.execute_reply": "2022-05-11T13:42:46.857011Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-11T13:42:46.859110Z",
     "iopub.execute_input": "2022-05-11T13:42:46.859598Z",
     "iopub.status.idle": "2022-05-11T13:42:47.912943Z",
     "shell.execute_reply.started": "2022-05-11T13:42:46.859568Z",
     "shell.execute_reply": "2022-05-11T13:42:47.912222Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "URL_REGEX = '(http|https)://[-a-zA-Z0-9+&@#/%?=~_|!:,.;]*[-a-zA-Z0-9+&@#/%=~_|]'\n",
    "NON_ALPHA_NUMERIC_REGEX = '[^a-zA-Z0-9- ]'\n",
    "TAG_REGEX = '@[^\\\\s]*'\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "AHOLE_CLASSES = ['yta', 'nta']\n",
    "MAX_SEQ_LENGTH = 512\n",
    "\n",
    "\n",
    "def load_dataset(filepath: str, classes: list, tokenizer, rm_punct: bool = False):\n",
    "    texts, labels = [], []\n",
    "    df = pd.read_csv(filepath)\n",
    "    df = df.head(5000)\n",
    "    \n",
    "    df_yta = df[df['is_yta'] == 'yta']\n",
    "    df_nta = df[df['is_yta'] == 'nta']\n",
    "    n = df_yta.shape[0]\n",
    "    df_nta = df_nta.iloc[:n]\n",
    "    \n",
    "    df = pd.concat([df_yta, df_nta], axis=0)\n",
    "    print(df)\n",
    "\n",
    "    \n",
    "    df['body'] = df['body'].astype(\"str\")\n",
    "    df['is_yta'] = df['is_yta'] == \"yta\"\n",
    "    for index, row in df.iterrows():\n",
    "        label = row['is_yta']\n",
    "        text = row['body']\n",
    "        tokens = process_text(text, tokenizer, rm_punct)\n",
    "        if len(tokens) > 3:\n",
    "            texts.append(tokens)\n",
    "            labels.append([label, not label])\n",
    "\n",
    "    return texts, labels\n",
    "def process_text(text, tokenizer, rm_punct: bool = False):\n",
    "    if type(text) is not str:\n",
    "        raise TypeError('Text is not of type string')\n",
    "\n",
    "    # remove special string from the text: URLs and emojis (by encoding and decoding to/from ascii)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_tags(text)\n",
    "\n",
    "    # remove unknown characters\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    # remove all punctuation\n",
    "    if rm_punct:\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # remove all labels from texts\n",
    "    tokens = delete_label_word(tokens, 'YTA')\n",
    "    tokens = delete_label_word(tokens, 'NTA')\n",
    "\n",
    "    tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def remove_non_alphanumeric(text: str) -> str:\n",
    "    return re.sub(NON_ALPHA_NUMERIC_REGEX, ' ', text)\n",
    "\n",
    "\n",
    "def remove_urls(text: str) -> str:\n",
    "    return re.sub(URL_REGEX, ' ', text)\n",
    "\n",
    "\n",
    "def remove_tags(text: str) -> str:\n",
    "    return re.sub(TAG_REGEX, ' ', text)\n",
    "\n",
    "\n",
    "def delete_label_word(words: list, label: str) -> list:\n",
    "    while label in words:\n",
    "        words.remove(label)\n",
    "    return words"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-11T13:42:47.915438Z",
     "iopub.execute_input": "2022-05-11T13:42:47.915814Z",
     "iopub.status.idle": "2022-05-11T13:42:47.931167Z",
     "shell.execute_reply.started": "2022-05-11T13:42:47.915773Z",
     "shell.execute_reply": "2022-05-11T13:42:47.930481Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "texts, labels = load_dataset(filepath='../input/reddit/posts.csv',\n",
    "                             classes=AHOLE_CLASSES,\n",
    "                             tokenizer=tokenizer,\n",
    "                             rm_punct=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-11T13:42:47.932623Z",
     "iopub.execute_input": "2022-05-11T13:42:47.932911Z",
     "iopub.status.idle": "2022-05-11T13:43:14.735776Z",
     "shell.execute_reply.started": "2022-05-11T13:42:47.932875Z",
     "shell.execute_reply": "2022-05-11T13:43:14.735027Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "input_ids, attention_masks = [], []\n",
    "for text in texts:\n",
    "    encoded_dict = tokenizer.encode_plus(text,\n",
    "                                         add_special_tokens=False,\n",
    "                                         truncation=True,\n",
    "                                         max_length=MAX_SEQ_LENGTH,\n",
    "                                         pad_to_max_length=True,\n",
    "                                         return_attention_mask=True,\n",
    "                                         return_tensors='pt')\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-11T13:43:14.736939Z",
     "iopub.execute_input": "2022-05-11T13:43:14.737606Z",
     "iopub.status.idle": "2022-05-11T13:43:16.626693Z",
     "shell.execute_reply.started": "2022-05-11T13:43:14.737566Z",
     "shell.execute_reply": "2022-05-11T13:43:16.625957Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-11T13:43:16.628096Z",
     "iopub.execute_input": "2022-05-11T13:43:16.628341Z",
     "iopub.status.idle": "2022-05-11T13:43:16.669415Z",
     "shell.execute_reply.started": "2022-05-11T13:43:16.628307Z",
     "shell.execute_reply": "2022-05-11T13:43:16.668708Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "labels = labels.bool().int().float()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-05-11T13:43:16.670535Z",
     "iopub.execute_input": "2022-05-11T13:43:16.670778Z",
     "iopub.status.idle": "2022-05-11T13:43:16.677343Z",
     "shell.execute_reply.started": "2022-05-11T13:43:16.670746Z",
     "shell.execute_reply": "2022-05-11T13:43:16.676622Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "input_ids.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-05-11T13:43:16.678477Z",
     "iopub.execute_input": "2022-05-11T13:43:16.678727Z",
     "iopub.status.idle": "2022-05-11T13:43:16.688411Z",
     "shell.execute_reply.started": "2022-05-11T13:43:16.678693Z",
     "shell.execute_reply": "2022-05-11T13:43:16.687547Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "train_size = int(0.75 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,  # The training samples.\n",
    "                              sampler=RandomSampler(train_dataset),  # Select batches randomly\n",
    "                              batch_size=batch_size)\n",
    "\n",
    "validation_dataloader = DataLoader(val_dataset,  # The validation samples.\n",
    "                                   sampler=SequentialSampler(val_dataset),  # Pull out batches sequentially.\n",
    "                                   batch_size=batch_size)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-11T13:43:16.691587Z",
     "iopub.execute_input": "2022-05-11T13:43:16.691943Z",
     "iopub.status.idle": "2022-05-11T13:43:16.706179Z",
     "shell.execute_reply.started": "2022-05-11T13:43:16.691907Z",
     "shell.execute_reply": "2022-05-11T13:43:16.705042Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "validation_dataloader.dataset[3]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-05-11T13:43:16.707524Z",
     "iopub.execute_input": "2022-05-11T13:43:16.707763Z",
     "iopub.status.idle": "2022-05-11T13:43:16.754551Z",
     "shell.execute_reply.started": "2022-05-11T13:43:16.707732Z",
     "shell.execute_reply": "2022-05-11T13:43:16.753743Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=len(AHOLE_CLASSES),\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=2e-5,  # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps=1e-8  # args.adam_epsilon  - default is 1e-8.\n",
    "                  )"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-11T13:43:16.756524Z",
     "iopub.execute_input": "2022-05-11T13:43:16.757157Z",
     "iopub.status.idle": "2022-05-11T13:43:33.235420Z",
     "shell.execute_reply.started": "2022-05-11T13:43:16.757118Z",
     "shell.execute_reply": "2022-05-11T13:43:33.234568Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "epochs = 4\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=100,\n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten().astype(np.float32)\n",
    "    labels_flat = labels[:,1].flatten().astype(np.float32)\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-11T13:45:15.261297Z",
     "iopub.execute_input": "2022-05-11T13:45:15.261551Z",
     "iopub.status.idle": "2022-05-11T13:45:15.273525Z",
     "shell.execute_reply.started": "2022-05-11T13:45:15.261523Z",
     "shell.execute_reply": "2022-05-11T13:45:15.272828Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# TRAINING\n# Tell pytorch to run this model on the GPU.\nmodel.cuda()\n\nseed_val = 42\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\ntraining_stats = []\ntotal_t0 = time.time()\n\nbest_val_acc, best_val_loss = 0, torch.finfo(torch.float32).max\n\nfor epoch_i in range(0, epochs):\n\n    # ========================================\n    #               Training\n    # ========================================\n\n    # Perform one full pass over the training set.\n\n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n\n    # Measure how long the training epoch takes.\n    t0 = time.time()\n\n    # Reset the total loss for this epoch.\n    total_train_loss = 0\n    total_train_accuracy = 0\n\n    model.train()\n\n    # For each batch of training data...\n    for step, batch in enumerate(train_dataloader):\n\n        # Progress update every 400 batches.\n        if step % 10 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n\n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        # Unpack this training batch from our dataloader.\n        #\n        # As we unpack the batch, we'll also copy each tensor to the GPU using the\n        # `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids\n        #   [1]: attention masks\n        #   [2]: labels\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        model.zero_grad()\n\n        loss, logits = model(b_input_ids,\n                             token_type_ids=None,\n                             attention_mask=b_input_mask,\n                             labels=b_labels,\n                             return_dict=False)\n\n        total_train_loss += loss.item()\n\n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        \n        # Calculate the accuracy for this batch of train sentences, and accumulate it over all batches.\n        total_train_accuracy += flat_accuracy(logits, label_ids)\n\n        # Perform a backward pass to calculate the gradients.\n        loss.backward()\n\n        # Clip the norm of the gradients to 1.0.\n        # This is to help prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # Update parameters and take a step using the computed gradient.\n        optimizer.step()\n\n        # Update the learning rate.\n        scheduler.step()\n\n    # Report the final accuracy for this train run.\n    avg_train_accuracy = total_train_accuracy / len(train_dataloader)\n    # Calculate the average loss over all of the batches.\n    avg_train_loss = total_train_loss / len(train_dataloader)\n    # Measure how long this epoch took.\n    training_time = format_time(time.time() - t0)\n\n    print(\"\")\n    print(\"  Training accuracy: {0:.4f}\".format(avg_train_accuracy))\n    print(\"  Average training loss: {0:.4f}\".format(avg_train_loss))\n    print(\"  Training epcoh took: {:}\".format(training_time))\n\n    # ========================================\n    #               Validation\n    # ========================================\n    # After the completion of each training epoch, measure our performance on our validation set.\n\n    print(\"\")\n    print(\"Running Validation...\")\n\n    t0 = time.time()\n\n    # Put the model in evaluation mode -- the dropout layers behave differently during evaluation.\n    model.eval()\n\n    # Tracking variables\n    total_eval_accuracy = 0\n    total_eval_loss = 0\n    nb_eval_steps = 0\n\n    # Evaluate data for one epoch\n    for batch in validation_dataloader:\n        # Unpack this training batch from our dataloader.\n        #\n        # As we unpack the batch, we'll also copy each tensor to the GPU using\n        # the `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids\n        #   [1]: attention masks\n        #   [2]: labels\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        # Tell pytorch not to bother with constructing the compute graph during\n        # the forward pass, since this is only needed for backprop (training).\n        with torch.no_grad():\n            # Forward pass, calculate logit predictions.\n            # token_type_ids is the same as the \"segment ids\", which\n            # differentiates sentence 1 and 2 in 2-sentence tasks.\n            # The documentation for this `model` function is here:\n            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n            # Get the \"logits\" output by the model. The \"logits\" are the output\n            # values prior to applying an activation function like the softmax.\n            (loss, logits) = model(b_input_ids,\n                                   token_type_ids=None,\n                                   attention_mask=b_input_mask,\n                                   labels=b_labels,\n                                   return_dict=False)\n\n        # Accumulate the validation loss.\n        total_eval_loss += loss.item()\n\n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n\n        # Calculate the accuracy for this batch of test sentences, and\n        # accumulate it over all batches.\n        total_eval_accuracy += flat_accuracy(logits, label_ids)\n\n    # Report the final accuracy for this validation run.\n    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n    # Calculate the average loss over all of the batches.\n    avg_val_loss = total_eval_loss / len(validation_dataloader)\n    # Measure how long the validation run took.\n    validation_time = format_time(time.time() - t0)\n\n    # save the model with the best accuracy and minimal loss (so we do not save an overfitted model)\n    if avg_val_accuracy > best_val_acc and avg_val_loss < best_val_loss:\n        best_val_acc = avg_val_accuracy\n        best_val_loss = avg_val_loss\n        best_model = copy.deepcopy(model)\n        best_epoch = epoch_i\n\n    print(\"  Validation Acc.: {0:.4f}\".format(avg_val_accuracy))\n    print(\"  Validation Loss: {0:.4f}\".format(avg_val_loss))\n    print(\"  Validation took: {:}\".format(validation_time))\n\n    # Record all statistics from this epoch.\n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Training Acc.': avg_train_accuracy * 100,\n            'Valid. Loss': avg_val_loss,\n            'Valid. Acc.': avg_val_accuracy * 100,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    )\n\nprint(\"\")\nprint(\"Training complete!\")\n\nprint(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time() - total_t0)))",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "execution": {
     "iopub.status.busy": "2022-05-11T13:45:17.715304Z",
     "iopub.execute_input": "2022-05-11T13:45:17.715563Z",
     "iopub.status.idle": "2022-05-11T13:56:22.642272Z",
     "shell.execute_reply.started": "2022-05-11T13:45:17.715534Z",
     "shell.execute_reply": "2022-05-11T13:56:22.641507Z"
    },
    "trusted": true
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "best_model.save_pretrained('/models')\nprint('Best fit model saved at epoch %d' % (best_epoch + 1))",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "execution": {
     "iopub.status.busy": "2022-05-11T15:12:18.801512Z",
     "iopub.execute_input": "2022-05-11T15:12:18.802256Z",
     "iopub.status.idle": "2022-05-11T15:12:18.907445Z",
     "shell.execute_reply.started": "2022-05-11T15:12:18.802133Z",
     "shell.execute_reply": "2022-05-11T15:12:18.905957Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\n\n# Display floats with two decimal places.\npd.set_option('precision', 4)\n\n# Create a DataFrame from our training statistics.\ndf_stats = pd.DataFrame(data=training_stats)\n\n# Use the 'epoch' as the row index.\ndf_stats = df_stats.set_index('epoch')\n\n# A hack to force the column headers to wrap.\n#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n\n# Display the table.\ndf_stats",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "execution": {
     "iopub.status.busy": "2022-05-11T15:12:18.909044Z",
     "iopub.status.idle": "2022-05-11T15:12:18.909834Z",
     "shell.execute_reply.started": "2022-05-11T15:12:18.909481Z",
     "shell.execute_reply": "2022-05-11T15:12:18.909515Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\n\nimport seaborn as sns\n\n# Use plot styling from seaborn.\nsns.set(style='darkgrid')\n\n# Increase the plot size and font size.\nsns.set(font_scale=1.5)\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\n# Plot the learning curve.\nplt.plot(df_stats['Training Loss'], 'b-o', label=\"Training Loss\")\nplt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation Loss\")\n\n# Label the plot.\nplt.title(\"Training & Validation Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.xticks(list(range(1, epochs + 1)))\n\nplt.show()",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "execution": {
     "iopub.status.busy": "2022-05-11T13:45:12.178548Z",
     "iopub.status.idle": "2022-05-11T13:45:12.179240Z",
     "shell.execute_reply.started": "2022-05-11T13:45:12.178973Z",
     "shell.execute_reply": "2022-05-11T13:45:12.179005Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}